# Requirements Document

## Introduction

The Voice AI Performance Optimizer (Agent Performance Copilot) is a tool that automates the testing and optimization of HighLevel Voice AI agents. It replaces manual trial-and-error prompt engineering with an AI-driven "Validation Flywheel" that autonomously generates test cases, evaluates agent responses against success criteria, and refines agent prompts to improve performance. The system integrates directly into the HighLevel interface as a custom widget and consists of a Vue.js frontend dashboard and a Node.js backend.

## Glossary

- **Copilot**: The Voice AI Performance Optimizer system that orchestrates testing and optimization
- **Voice_AI_Agent**: A HighLevel Voice AI agent configured with a base prompt/script to handle voice interactions
- **Test_Case**: A simulated user input scenario generated by the Copilot to evaluate a Voice AI Agent's behavior
- **Success_Criterion**: A measurable KPI that defines expected behavior for a Voice AI Agent response (e.g., "Must ask for an email address")
- **Test_Suite**: A collection of Test Cases and their associated Success Criteria targeting a specific Voice AI Agent
- **Test_Run**: A single execution of a Test Suite against a Voice AI Agent, producing scored results
- **Optimization_Cycle**: A complete loop of testing, analyzing results, and refining the Voice AI Agent's base prompt
- **Base_Prompt**: The primary prompt/script that governs a Voice AI Agent's conversational behavior
- **Widget**: The custom JavaScript component embedded in the HighLevel interface that hosts the Copilot UI
- **HighLevel_API**: The HighLevel platform APIs used to read and update Voice AI Agent configurations
- **LLM_Service**: The large language model service used by the Copilot for test generation, result analysis, and prompt optimization

## Requirements

### Requirement 1: Voice AI Agent Prompt Analysis

**User Story:** As a HighLevel agency user, I want the Copilot to analyze my Voice AI agent's existing prompt, so that it can understand the agent's intended behavior and generate relevant tests.

#### Acceptance Criteria

1. WHEN a user selects a Voice AI Agent for optimization, THE Copilot SHALL retrieve the agent's Base Prompt from the HighLevel API
2. WHEN the Copilot receives a Base Prompt, THE LLM_Service SHALL extract the agent's intended goals, conversation flow, and expected behaviors into a structured analysis
3. WHEN the prompt analysis completes, THE Copilot SHALL display a summary of identified goals and behaviors to the user
4. IF the HighLevel API is unreachable or returns an error, THEN THE Copilot SHALL display a descriptive error message and allow the user to retry

### Requirement 2: Automated Test Case Generation

**User Story:** As a HighLevel agency user, I want the Copilot to automatically generate test cases from my agent's prompt, so that I can validate agent behavior without manually writing scenarios.

#### Acceptance Criteria

1. WHEN prompt analysis is complete, THE Copilot SHALL generate a set of Test Cases covering the identified goals and conversation flows
2. THE Copilot SHALL generate Test Cases that include both happy-path scenarios and adversarial scenarios (e.g., user interruptions, off-topic questions, refusal to provide information)
3. WHEN generating Test Cases, THE Copilot SHALL produce a minimum of five Test Cases per Voice AI Agent
4. WHEN a Test Case is generated, THE Copilot SHALL include a simulated user input sequence and a description of the scenario being tested
5. WHEN Test Cases are generated, THE Copilot SHALL allow the user to review, edit, add, or remove individual Test Cases before execution

### Requirement 3: Success Criteria Definition

**User Story:** As a HighLevel agency user, I want the Copilot to define measurable success criteria for each test case, so that agent responses can be objectively evaluated.

#### Acceptance Criteria

1. WHEN generating Test Cases, THE Copilot SHALL also generate at least one Success Criterion per Test Case
2. THE Copilot SHALL express each Success Criterion as a measurable, evaluable statement (e.g., "Agent asks for the caller's email address," "Agent maintains a polite tone throughout the conversation")
3. WHEN Success Criteria are generated, THE Copilot SHALL allow the user to review, edit, add, or remove individual Success Criteria before execution
4. THE Copilot SHALL categorize each Success Criterion by type: behavioral (tone, politeness), functional (collects required info), or compliance (follows script structure)

### Requirement 4: Test Execution

**User Story:** As a HighLevel agency user, I want the Copilot to execute test cases against my Voice AI agent, so that I can see how the agent performs under various scenarios.

#### Acceptance Criteria

1. WHEN the user initiates a Test Run, THE Copilot SHALL execute each Test Case in the Test Suite against the Voice AI Agent sequentially
2. WHEN executing a Test Case, THE Copilot SHALL simulate the user input sequence and capture the Voice AI Agent's full response
3. WHEN a Test Case execution completes, THE LLM_Service SHALL evaluate the Voice AI Agent's response against each associated Success Criterion and assign a pass or fail result
4. WHEN all Test Cases in a Test Run complete, THE Copilot SHALL calculate an overall pass rate as the percentage of Success Criteria that passed
5. IF a Test Case execution fails due to a system error (timeout, API failure), THEN THE Copilot SHALL mark that Test Case as "error" rather than "fail" and allow the user to retry the individual Test Case

### Requirement 5: Results Dashboard

**User Story:** As a HighLevel agency user, I want to see a clear dashboard of test results, so that I can understand where my agent is performing well and where it needs improvement.

#### Acceptance Criteria

1. WHEN a Test Run completes, THE Copilot SHALL display a results dashboard showing the overall pass rate, per-test-case results, and per-criterion pass/fail status
2. WHEN displaying results, THE Copilot SHALL show the Voice AI Agent's actual response alongside each Success Criterion evaluation
3. WHEN displaying a failed criterion, THE Copilot SHALL include a brief explanation of why the criterion was not met
4. WHEN multiple Test Runs have been completed for the same Voice AI Agent, THE Copilot SHALL display a comparison view showing performance trends across runs

### Requirement 6: Automated Prompt Optimization

**User Story:** As a HighLevel agency user, I want the Copilot to automatically refine my agent's prompt based on test failures, so that I can improve agent performance without manual prompt engineering.

#### Acceptance Criteria

1. WHEN a Test Run produces failures, THE Copilot SHALL analyze the failed criteria and generate a revised Base Prompt using the LLM_Service
2. WHEN generating a revised Base Prompt, THE LLM_Service SHALL target the specific failures while preserving passing behaviors
3. WHEN a revised Base Prompt is generated, THE Copilot SHALL display a diff view showing changes between the original and revised prompts
4. WHEN a revised Base Prompt is generated, THE Copilot SHALL allow the user to accept, reject, or manually edit the revised prompt before applying
5. WHEN the user accepts a revised Base Prompt, THE Copilot SHALL update the Voice AI Agent's configuration via the HighLevel API
6. IF the prompt update via the HighLevel API fails, THEN THE Copilot SHALL retain the revised prompt locally and display an error message with retry option

### Requirement 7: Optimization Cycle Loop

**User Story:** As a HighLevel agency user, I want the Copilot to run multiple test-optimize cycles automatically, so that the agent's prompt converges toward optimal performance.

#### Acceptance Criteria

1. WHEN the user enables auto-cycle mode, THE Copilot SHALL repeatedly execute Test Runs and Optimization Cycles until the overall pass rate meets a user-defined threshold or a maximum cycle count is reached
2. WHILE an auto-cycle is running, THE Copilot SHALL display the current cycle number, pass rate, and status in real time
3. WHEN an auto-cycle completes, THE Copilot SHALL present a summary showing the starting pass rate, ending pass rate, number of cycles executed, and all prompt revisions made
4. THE Copilot SHALL allow the user to pause or cancel an auto-cycle at any point

### Requirement 8: HighLevel Widget Integration

**User Story:** As a HighLevel agency user, I want the Copilot to be embedded directly in the HighLevel interface, so that I can access it without leaving my workflow.

#### Acceptance Criteria

1. THE Widget SHALL load within the HighLevel interface via custom JavaScript injection supported by HighLevel agency accounts
2. WHEN the Widget loads, THE Copilot SHALL authenticate with the backend using the current HighLevel session context
3. THE Widget SHALL render as a Vue.js application embedded in the HighLevel page without disrupting existing HighLevel UI elements
4. WHEN the HighLevel page navigates away and returns, THE Widget SHALL restore its previous state

### Requirement 9: Before vs. After Comparison

**User Story:** As a HighLevel agency user, I want to see a clear before-and-after comparison of my agent's performance, so that I can verify the value of the optimization.

#### Acceptance Criteria

1. WHEN at least one Optimization Cycle has completed, THE Copilot SHALL provide a "Before vs. After" view comparing the original and optimized Base Prompts side by side
2. WHEN displaying the Before vs. After view, THE Copilot SHALL show test result metrics (pass rate, per-criterion results) for both the original and optimized prompts
3. WHEN displaying the Before vs. After view, THE Copilot SHALL highlight specific improvements and regressions in the optimized prompt's performance
